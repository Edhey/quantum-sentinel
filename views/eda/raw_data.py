import streamlit as st
import pandas as pd
import numpy as np
from utils import FILE_PATH

st.title("üìÑ Exploration Data Analysis (EDA)")
st.markdown(
    """
In this secction we perform the initial loading of the dataset generated by the quantum 
simulation and verify its structural integrity before moving on to visual analysis.
"""
)


def simulate_bb84_transmission(n_bits=128, intercept_prob=0.0, noise_level=0.0):
    """Simula una transmisi√≥n BB84 completa y devuelve las m√©tricas."""
    alice_bits = np.random.randint(0, 2, n_bits)
    alice_bases = np.random.randint(0, 2, n_bits)
    bob_bases = np.random.randint(0, 2, n_bits)
    bob_measurements = np.zeros(n_bits, dtype=int)

    for i in range(n_bits):
        bit = alice_bits[i]
        base_a = alice_bases[i]

        # Eva
        if np.random.rand() < intercept_prob:
            eve_base = np.random.randint(0, 2)
            if eve_base != base_a:
                bit = np.random.randint(0, 2)

        # Noise
        if np.random.rand() < noise_level:
            bit = 1 - bit

        # Bob
        if base_a == bob_bases[i]:
            bob_measurements[i] = bit
        else:
            bob_measurements[i] = np.random.randint(0, 2)

    # Sifting
    match_indices = np.where(alice_bases == bob_bases)[0]
    sifted_alice = alice_bits[match_indices]
    sifted_bob = bob_measurements[match_indices]

    # Extract Features
    sifted_length = len(sifted_alice)
    if sifted_length == 0:
        return None

    errors = np.sum(sifted_alice != sifted_bob)
    qber = errors / sifted_length
    basis_match_rate = sifted_length / n_bits

    return {
        "n_bits": n_bits,
        "qber": qber,
        "sifted_count": sifted_length,
        "basis_match_rate": basis_match_rate,
        "noise_level": noise_level,
        "attacker_present": 1 if intercept_prob > 0 else 0,
    }


def generate_new_dataset(n_samples=5000, n_bits=512):
    data = []
    progress_bar = st.progress(0)

    # Secure Traffic Scenario
    for i in range(n_samples):
        noise = np.random.uniform(0.0, 0.05)
        sim = simulate_bb84_transmission(
            n_bits=n_bits, intercept_prob=0.0, noise_level=noise
        )
        if sim:
            data.append(sim)
        if i % 500 == 0:
            progress_bar.progress((i / (n_samples * 2)))

    # Eve Attack Scenario
    for i in range(n_samples):
        noise = np.random.uniform(0.0, 0.05)
        eve_aggressiveness = np.random.uniform(0.1, 1.0)
        sim = simulate_bb84_transmission(
            n_bits=n_bits, intercept_prob=eve_aggressiveness, noise_level=noise
        )
        if sim:
            data.append(sim)
        if i % 500 == 0:
            progress_bar.progress(0.5 + (i / (n_samples * 2)))

    progress_bar.empty()
    return pd.DataFrame(data)


st.subheader("1. Dataset Management")


@st.cache_data
def load_data():
    try:
        return pd.read_csv(FILE_PATH)
    except FileNotFoundError:
        return None


df = load_data()

col_status, col_action = st.columns([2, 1])

with col_status:
    if df is None:
        st.warning("Dataset not found. Please generate it first.")
        file_exists = False
    else:
        st.success(f"Dataset found: `{FILE_PATH}`")
        file_exists = True

with col_action:
    input_samples = st.slider(
        "Samples per Scenario",
        min_value=1000,
        max_value=10000,
        value=5000,
        step=1000,
        help="Samples for Safe Traffic + Samples for Attack. Total will be 2x this value.",
    )
    total_sims = input_samples * 2
    input_bits = st.slider(
        "Bits per Transmission",
        min_value=128,
        max_value=1024,
        value=512,
        step=128,
        help="Raw key length transmitted in each simulation.",
    )

    btn_label = "üîÑ Regenerate Dataset" if file_exists else "‚öôÔ∏è Generate Dataset"
    if st.button(btn_label, type="primary" if not file_exists else "secondary"):
        with st.spinner(f"Simulating {total_sims:,} quantum transmissions..."):
            df_new = generate_new_dataset(n_samples=input_samples, n_bits=input_bits)
            df_new.to_csv(FILE_PATH, index=False)
        st.success("Dataset generated successfully!")
        st.cache_data.clear()
        st.rerun()

if not file_exists:
    st.info("Click the button above to start the dataset generation process.")
    st.stop()


# Variable dictionary
with st.expander("Variable Dictionary", expanded=True):
    st.markdown(
        """
    For understanding this dataset, we define the metrics of the BB84 protocol:
    
    * **`n_bits`**: Key length (number of bits transmitted).
    * **`qber`**: Quantum Bit Error Rate, which indicates the error rate in the transmission. 
        * *Formula:* $Error / Shifted\_Bits$. 
        * *Theory:* Noise + Eve's Interference.
    * **`sifted_count`**: Number of bits that survived the sifting process (where Alice and Bob matched bases).
    * **`basis_match_rate`**: Percentage of basis matches. Should be around 50% by chance.
    * **`attacker_present` (Target)**: Target variable.
        * `0`: Secure Channel (Only natural noise).
        * `1`: Active Attack (Eve intercepting).
    """
    )

st.divider()

# Dataset overview
st.subheader("1. Dataset Overview")
st.caption(f"Dataset dimensions: {df.shape[0]} rows x {df.shape[1]} columns")

if st.checkbox("Show Full Dataset"):
    st.dataframe(df)
else:
    st.dataframe(df.head(10))

st.divider()

# Statistical summary
st.subheader("2. Statistical Summary")
st.markdown("Analysis of the numerical distribution of variables.")

st.dataframe(df.describe().T.style.background_gradient(cmap="Blues"))

st.markdown(
    """
We can see that our data generation process has successfully created a dataset 
that adheres to the expected physical laws of the BB84 protocol.
- The `basis_match_rate` is around 50%, which is consistent with the theoretical expectations for random basis selection.
- The dataset has a perfect balance of noise and attack scenarios (50/50).
- The `qber` values indicate that we have a realistic range of error rates.
"""
)

st.divider()

# Data Quality Checks
st.subheader("3. Data Quality Checks")

col1, col2 = st.columns(2)

with col1:
    st.markdown("**Null Values (NaN)**")
    nulls = df.isnull().sum()
    if nulls.sum() == 0:
        st.success("There are no null values in the dataset.")
    else:
        st.warning("Null values detected:")
        st.write(nulls)

with col2:
    st.markdown("**Duplicate Rows**")
    dupes = df.duplicated().sum()
    if dupes == 0:
        st.success("There are no duplicate rows in the dataset.")
    else:
        st.info(f"There are {dupes} rows with identical values.")
        st.caption(
            "(This is normal in simulations if parameters are repeated, not necessarily an error)."
        )

# Class Balance Check
st.subheader("4. Class Balance Check (Target)")
conteo = df["attacker_present"].value_counts()
st.write(conteo)

if abs(conteo[0] - conteo[1]) < (len(df) * 0.1):
    st.success("The dataset is balanced. Ideal for training.")
else:
    st.warning("The dataset is unbalanced. Consider resampling techniques.")

# Conclusion
st.subheader("5. Conclusion of Basic Statistics")
st.markdown(
    """
Based on the **Statistical Summary**, we identified that **QBER** is the feature with the 
highest relative variability. This high variance suggests that QBER contains the most
significant information to distinguish between classes.

Also our generated dataset is balanced and clean, with no null values or duplicates, 
which is ideal for training our ML model in the next steps.
"""
)
